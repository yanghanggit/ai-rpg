#!/usr/bin/env python3
"""
Azure OpenAI Chat Serverå¯åŠ¨è„šæœ¬

åŠŸèƒ½ï¼š
1. åŸºäºFastAPIæ„å»ºçš„Azure OpenAI GPTèŠå¤©æœåŠ¡å™¨
2. æä¾›RESTful APIæ¥å£
3. æ”¯æŒèŠå¤©å†å²å’Œä¸Šä¸‹æ–‡è®°å¿†
4. å¼‚æ­¥å¤„ç†èŠå¤©è¯·æ±‚

ä½¿ç”¨æ–¹æ³•ï¼š
    python scripts/run_azure_openai_chat_server.py

æˆ–è€…åœ¨é¡¹ç›®æ ¹ç›®å½•ä¸‹ï¼š
    python -m scripts.run_azure_openai_chat_server

APIç«¯ç‚¹ï¼š
    POST /chat-service/v1/
"""

import os
import sys

# å°† src ç›®å½•æ·»åŠ åˆ°æ¨¡å—æœç´¢è·¯å¾„
sys.path.insert(
    0, os.path.join(os.path.dirname(os.path.abspath(__file__)), "..", "src")
)

from fastapi import FastAPI
from loguru import logger

from multi_agents_game.chat_services.chat_api import ChatRequest, ChatResponse
from multi_agents_game.azure_openai_gpt import (
    State,
    create_compiled_stage_graph,
    stream_graph_updates,
    create_azure_openai_gpt_llm,
)

##################################################################################################################
# åˆå§‹åŒ– FastAPI åº”ç”¨
app = FastAPI(
    title="Azure OpenAI Chat Server",
    description="åŸºäºAzure OpenAI GPTçš„èŠå¤©æœåŠ¡å™¨",
    version="1.0.0",
)


##################################################################################################################
# å®šä¹‰ POST è¯·æ±‚å¤„ç†é€»è¾‘
@app.post(
    path="/chat-service/v1/",
    response_model=ChatResponse,
)
async def process_chat_request(request: ChatRequest) -> ChatResponse:
    """
    å¤„ç†èŠå¤©è¯·æ±‚

    Args:
        request: åŒ…å«èŠå¤©å†å²å’Œç”¨æˆ·æ¶ˆæ¯çš„è¯·æ±‚å¯¹è±¡

    Returns:
        ChatResponse: åŒ…å«AIå›å¤æ¶ˆæ¯çš„å“åº”å¯¹è±¡
    """
    try:
        logger.info(f"æ”¶åˆ°èŠå¤©è¯·æ±‚: {request.message.content}")

        # ä¸ºæ¯ä¸ªè¯·æ±‚åˆ›å»ºç‹¬ç«‹çš„LLMå®ä¾‹
        llm = create_azure_openai_gpt_llm()

        # ä¸ºæ¯ä¸ªè¯·æ±‚åˆ›å»ºç‹¬ç«‹çš„çŠ¶æ€å›¾å®ä¾‹
        compiled_state_graph = create_compiled_stage_graph(
            "azure_chat_openai_chatbot_node"
        )

        # èŠå¤©å†å²ï¼ˆåŒ…å«LLMå®ä¾‹ï¼‰
        chat_history_state: State = {
            "messages": [message for message in request.chat_history],
            "llm": llm,
        }

        # ç”¨æˆ·è¾“å…¥
        user_input_state: State = {"messages": [request.message], "llm": llm}

        # è·å–å›å¤
        update_messages = stream_graph_updates(
            state_compiled_graph=compiled_state_graph,
            chat_history_state=chat_history_state,
            user_input_state=user_input_state,
        )

        logger.success(f"ç”Ÿæˆå›å¤æ¶ˆæ¯æ•°é‡: {len(update_messages)}")

        # è¿”å›
        return ChatResponse(messages=update_messages)

    except Exception as e:
        logger.error(f"å¤„ç†èŠå¤©è¯·æ±‚æ—¶å‘ç”Ÿé”™è¯¯: {e}")
        # è¿”å›é”™è¯¯æ¶ˆæ¯
        from langchain.schema import AIMessage

        error_message = AIMessage(content=f"æŠ±æ­‰ï¼Œå¤„ç†æ‚¨çš„è¯·æ±‚æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
        return ChatResponse(messages=[error_message])


##################################################################################################################
def main() -> None:
    """
    Azure OpenAIèŠå¤©æœåŠ¡å™¨ä¸»å‡½æ•°

    åŠŸèƒ½ï¼š
    1. å¯åŠ¨FastAPIæœåŠ¡å™¨
    2. é…ç½®æœåŠ¡å™¨å‚æ•°
    3. æä¾›èŠå¤©APIæœåŠ¡
    """
    logger.info("ğŸš€ å¯åŠ¨Azure OpenAIèŠå¤©æœåŠ¡å™¨...")

    try:
        import uvicorn

        # å¯åŠ¨æœåŠ¡å™¨
        uvicorn.run(
            app,
            host="localhost",
            port=8100,
            log_level="debug",
        )

    except Exception as e:
        logger.error(f"âŒ å¯åŠ¨æœåŠ¡å™¨å¤±è´¥: {e}")
        raise


##################################################################################################################
if __name__ == "__main__":
    main()
