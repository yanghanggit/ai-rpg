#!/usr/bin/env python3
"""
Replicate å¯¹è¯å·¥å…·
ä¸€ä¸ªç®€å•æ˜“ç”¨çš„å¯¹è¯è„šæœ¬ï¼Œæ”¯æŒå¤šç§LLMæ¨¡å‹è¿›è¡Œå¯¹è¯
"""

import argparse
import os
import sys
import time
from typing import Dict, Final, List

import replicate
import requests
from dotenv import load_dotenv

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

# å†…ç½®å¯¹è¯æ¨¡å‹é…ç½®
CHAT_MODELS: Dict[str, Dict[str, str]] = {
    "gpt-4o-mini": {
        "version": "openai/gpt-4o-mini",
        "cost_estimate": "$0.15/1M input + $0.6/1M output tokens",
        "description": "OpenAI ä½æˆæœ¬é«˜æ•ˆå¯¹è¯æ¨¡å‹ï¼Œæ¨èæ—¥å¸¸ä½¿ç”¨",
    },
    "gpt-4o": {
        "version": "openai/gpt-4o",
        "cost_estimate": "$2.5/1M input + $10/1M output tokens",
        "description": "OpenAI é«˜æ™ºèƒ½å¤šæ¨¡æ€å¯¹è¯æ¨¡å‹",
    },
    "claude-3.5-sonnet": {
        "version": "anthropic/claude-3.5-sonnet",
        "cost_estimate": "ä¸­ç­‰æˆæœ¬ï¼Œé«˜è´¨é‡å¯¹è¯",
        "description": "Anthropic é«˜æ™ºèƒ½å¯¹è¯æ¨¡å‹ï¼Œæ“…é•¿åˆ†æå’Œæ¨ç†",
    },
    "llama-3.1-405b": {
        "version": "meta/meta-llama-3.1-405b-instruct",
        "cost_estimate": "å¼€æºå¤§æ¨¡å‹ï¼Œæˆæœ¬è¾ƒé«˜",
        "description": "Meta å¼€æºæ——èˆ°å¯¹è¯æ¨¡å‹",
    },
    "llama-3-70b": {
        "version": "meta/meta-llama-3-70b-instruct",
        "cost_estimate": "å¼€æºæ¨¡å‹ï¼Œå¹³è¡¡æ€§èƒ½å’Œæˆæœ¬",
        "description": "Meta å¼€æºå¯¹è¯æ¨¡å‹ï¼Œæ€§ä»·æ¯”é«˜",
    },
}

# å…¨å±€å˜é‡
API_TOKEN: str = os.getenv("REPLICATE_API_TOKEN") or ""
TEST_URL: Final[str] = "https://api.replicate.com/v1/models"
DEFAULT_MODEL: Final[str] = "gpt-4o-mini"


def test_connection() -> bool:
    """æµ‹è¯•è¿æ¥æ˜¯å¦æ­£å¸¸"""
    headers = {"Authorization": f"Token {API_TOKEN}"}

    try:
        print("ğŸ”„ æµ‹è¯• Replicate API è¿æ¥...")
        response = requests.get(TEST_URL, headers=headers, timeout=10)

        if response.status_code == 200:
            print("âœ… è¿æ¥æˆåŠŸ! Replicate API å¯æ­£å¸¸è®¿é—®")
            return True
        else:
            print(f"âŒ è¿æ¥å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code}")
            if response.status_code == 401:
                print("ğŸ’¡ API Token å¯èƒ½æ— æ•ˆæˆ–å·²è¿‡æœŸ")
            return False

    except Exception as e:
        print(f"âŒ è¿æ¥é”™è¯¯: {e}")
        print("ğŸ’¡ è¯·æ£€æŸ¥:")
        print("   1. ç½‘ç»œè¿æ¥æ˜¯å¦æ­£å¸¸")
        print("   2. API Token æ˜¯å¦æœ‰æ•ˆ")
        return False


def chat_single(
    prompt: str,
    model_name: str = DEFAULT_MODEL,
    system_prompt: str = "You are a helpful assistant.",
    max_tokens: int = 1000,
    temperature: float = 0.7,
    stream: bool = False,
) -> str:
    """
    å•æ¬¡å¯¹è¯

    Args:
        prompt: ç”¨æˆ·è¾“å…¥çš„æ¶ˆæ¯
        model_name: æ¨¡å‹åç§°
        system_prompt: ç³»ç»Ÿæç¤ºè¯
        max_tokens: æœ€å¤§tokenæ•°
        temperature: æ¸©åº¦å‚æ•°ï¼ˆ0-2ï¼‰
        stream: æ˜¯å¦æµå¼è¾“å‡º

    Returns:
        æ¨¡å‹å›å¤
    """
    if model_name not in CHAT_MODELS:
        raise ValueError(
            f"ä¸æ”¯æŒçš„æ¨¡å‹: {model_name}. å¯ç”¨æ¨¡å‹: {list(CHAT_MODELS.keys())}"
        )

    model_info = CHAT_MODELS[model_name]
    model_version = model_info["version"]
    cost_estimate = model_info["cost_estimate"]

    print(f"\nğŸ¤– ä½¿ç”¨æ¨¡å‹: {model_name}")
    print(f"ğŸ’° é¢„ä¼°æˆæœ¬: {cost_estimate}")
    print(f"ğŸ“ ç”¨æˆ·è¾“å…¥: {prompt[:80]}{'...' if len(prompt) > 80 else ''}")
    print("ğŸ”„ æ€è€ƒä¸­...")

    start_time = time.time()

    try:
        # æ„å»ºè¾“å…¥å‚æ•°
        input_params = {
            "prompt": prompt,
            "system_prompt": system_prompt,
            "max_completion_tokens": max_tokens,
            "temperature": temperature,
        }

        if stream:
            # æµå¼è¾“å‡º
            print("\nğŸ¯ AI å›å¤:")
            print("-" * 50)

            iterator = replicate.run(model_version, input=input_params)
            full_response = ""

            for text in iterator:
                print(text, end="", flush=True)
                full_response += text

            print()  # æ¢è¡Œ
            print("-" * 50)

            elapsed_time = time.time() - start_time
            print(f"â±ï¸  å®Œæˆæ—¶é—´: {elapsed_time:.2f}ç§’")

            return full_response
        else:
            # ä¸€æ¬¡æ€§è¾“å‡º
            output = replicate.run(model_version, input=input_params)

            response = ""
            if isinstance(output, list):
                response = "".join(str(item) for item in output)
            else:
                response = str(output)

            elapsed_time = time.time() - start_time
            print(f"\nğŸ¯ AI å›å¤:")
            print("-" * 50)
            print(response)
            print("-" * 50)
            print(f"â±ï¸  å®Œæˆæ—¶é—´: {elapsed_time:.2f}ç§’")

            return response

    except Exception as e:
        print(f"âŒ å¯¹è¯å¤±è´¥: {e}")
        raise


def chat_interactive(
    model_name: str = DEFAULT_MODEL,
    system_prompt: str = "You are a helpful assistant.",
    stream: bool = True,
) -> None:
    """
    äº¤äº’å¼å¯¹è¯æ¨¡å¼

    Args:
        model_name: æ¨¡å‹åç§°
        system_prompt: ç³»ç»Ÿæç¤ºè¯
        stream: æ˜¯å¦æµå¼è¾“å‡º
    """
    print("=" * 60)
    print("ğŸ® Replicate äº¤äº’å¼å¯¹è¯")
    print("=" * 60)
    print(f"ğŸ¤– å½“å‰æ¨¡å‹: {model_name}")
    print(f"ğŸ“‹ æ¨¡å‹æè¿°: {CHAT_MODELS[model_name]['description']}")
    print(f"ğŸ’° æˆæœ¬ä¼°ç®—: {CHAT_MODELS[model_name]['cost_estimate']}")
    print("\nğŸ’¡ ä½¿ç”¨è¯´æ˜:")
    print("   - ç›´æ¥è¾“å…¥æ¶ˆæ¯å¼€å§‹å¯¹è¯")
    print("   - è¾“å…¥ 'quit' æˆ– 'exit' é€€å‡º")
    print("   - è¾“å…¥ 'clear' æ¸…ç©ºå¯¹è¯å†å²")
    print("   - è¾“å…¥ 'help' æŸ¥çœ‹å¸®åŠ©")
    print("=" * 60)

    conversation_history: List[str] = []

    while True:
        try:
            # è·å–ç”¨æˆ·è¾“å…¥
            user_input = input("\nğŸ‘¤ ä½ : ").strip()

            if not user_input:
                continue

            # å¤„ç†ç‰¹æ®Šå‘½ä»¤
            if user_input.lower() in ["quit", "exit"]:
                print("ğŸ‘‹ å†è§!")
                break
            elif user_input.lower() == "clear":
                conversation_history.clear()
                print("ğŸ§¹ å¯¹è¯å†å²å·²æ¸…ç©º")
                continue
            elif user_input.lower() == "help":
                print("\nğŸ“– å¯ç”¨å‘½ä»¤:")
                print("   - quit/exit: é€€å‡ºå¯¹è¯")
                print("   - clear: æ¸…ç©ºå¯¹è¯å†å²")
                print("   - help: æ˜¾ç¤ºå¸®åŠ©")
                continue

            # æ·»åŠ åˆ°å¯¹è¯å†å²
            conversation_history.append(f"ç”¨æˆ·: {user_input}")

            # æ„å»ºå®Œæ•´çš„å¯¹è¯ä¸Šä¸‹æ–‡
            if len(conversation_history) > 1:
                context = "\n".join(conversation_history[-10:])  # ä¿ç•™æœ€è¿‘10è½®å¯¹è¯
                full_prompt = f"å¯¹è¯å†å²:\n{context}\n\nè¯·å›å¤æœ€æ–°çš„ç”¨æˆ·æ¶ˆæ¯ã€‚"
            else:
                full_prompt = user_input

            # å‘é€ç»™AI
            response = chat_single(
                prompt=full_prompt,
                model_name=model_name,
                system_prompt=system_prompt,
                stream=stream,
            )

            # æ·»åŠ AIå›å¤åˆ°å†å²
            conversation_history.append(f"AI: {response}")

        except KeyboardInterrupt:
            print("\n\nğŸ‘‹ ç”¨æˆ·ä¸­æ–­ï¼Œå†è§!")
            break
        except Exception as e:
            print(f"\nâŒ å¯¹è¯å‡ºé”™: {e}")


def run_demo() -> None:
    """è¿è¡Œæ¼”ç¤ºç¤ºä¾‹"""
    print("=" * 60)
    print("ğŸ® Replicate å¯¹è¯æ¼”ç¤º")
    print("=" * 60)

    # 1. æµ‹è¯•è¿æ¥
    if not test_connection():
        print("âŒ è¿æ¥æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè®¾ç½®")
        return

    # 2. æŸ¥çœ‹å¯ç”¨æ¨¡å‹
    print("\nğŸ“‹ å¯ç”¨å¯¹è¯æ¨¡å‹:")
    for name, info in CHAT_MODELS.items():
        cost = info["cost_estimate"]
        description = info["description"]
        print(f"  - {name}:")
        print(f"    ğŸ’° {cost}")
        print(f"    ğŸ“ {description}")

    # 3. æµ‹è¯•å¯¹è¯
    print("\nğŸ¤– æµ‹è¯•å¯¹è¯åŠŸèƒ½...")

    try:
        test_prompt = "ä½ å¥½ï¼è¯·ç®€å•ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±ã€‚"

        response = chat_single(
            prompt=test_prompt, model_name=DEFAULT_MODEL, stream=False
        )

        print(f"\nğŸ‰ æ¼”ç¤ºå®Œæˆ!")
        print("ğŸ’¡ æ‚¨å¯ä»¥ä½¿ç”¨ --interactive æ¨¡å¼è¿›è¡Œè¿ç»­å¯¹è¯")

    except Exception as e:
        print(f"âŒ æ¼”ç¤ºå¤±è´¥: {e}")


def main() -> None:
    """ä¸»å‡½æ•° - å‘½ä»¤è¡Œæ¥å£"""
    if not API_TOKEN:
        print("âŒ é”™è¯¯: API Token æœªé…ç½®")
        print("ğŸ’¡ è¯·æ£€æŸ¥:")
        print("   1. ç¯å¢ƒå˜é‡ REPLICATE_API_TOKEN æ˜¯å¦è®¾ç½®")
        print("   2. .env æ–‡ä»¶æ˜¯å¦å­˜åœ¨ä¸”åŒ…å«æ­£ç¡®çš„ API Token")
        sys.exit(1)

    parser = argparse.ArgumentParser(description="Replicate å¯¹è¯å·¥å…·")
    parser.add_argument("prompt", nargs="?", help="å¯¹è¯å†…å®¹")
    parser.add_argument(
        "--model",
        "-m",
        default=DEFAULT_MODEL,
        choices=list(CHAT_MODELS.keys()),
        help=f"ä½¿ç”¨çš„æ¨¡å‹ (é»˜è®¤: {DEFAULT_MODEL})",
    )
    parser.add_argument(
        "--system", "-s", default="You are a helpful assistant.", help="ç³»ç»Ÿæç¤ºè¯"
    )
    parser.add_argument(
        "--max-tokens", "-t", type=int, default=1000, help="æœ€å¤§tokenæ•° (é»˜è®¤: 1000)"
    )
    parser.add_argument(
        "--temperature", type=float, default=0.7, help="æ¸©åº¦å‚æ•° 0-2 (é»˜è®¤: 0.7)"
    )
    parser.add_argument("--no-stream", action="store_true", help="ç¦ç”¨æµå¼è¾“å‡º")
    parser.add_argument(
        "--interactive", "-i", action="store_true", help="è¿›å…¥äº¤äº’å¼å¯¹è¯æ¨¡å¼"
    )
    parser.add_argument("--list-models", action="store_true", help="åˆ—å‡ºå¯ç”¨æ¨¡å‹")
    parser.add_argument("--demo", action="store_true", help="è¿è¡Œæ¼”ç¤º")
    parser.add_argument("--test", action="store_true", help="æµ‹è¯•è¿æ¥")

    args = parser.parse_args()

    try:
        print("âœ… Replicate å®¢æˆ·ç«¯åˆå§‹åŒ–å®Œæˆ")

        # è¿è¡Œæ¼”ç¤º
        if args.demo:
            run_demo()
            return

        # æµ‹è¯•è¿æ¥
        if args.test:
            test_connection()
            return

        # åˆ—å‡ºæ¨¡å‹
        if args.list_models:
            print("ğŸ¤– å¯ç”¨å¯¹è¯æ¨¡å‹:")
            for name, info in CHAT_MODELS.items():
                cost = info["cost_estimate"]
                description = info["description"]
                print(f"  - {name}:")
                print(f"    ğŸ’° {cost}")
                print(f"    ğŸ“ {description}")
            return

        # äº¤äº’å¼æ¨¡å¼
        if args.interactive:
            chat_interactive(
                model_name=args.model,
                system_prompt=args.system,
                stream=not args.no_stream,
            )
            return

        # å¦‚æœæ²¡æœ‰æä¾›promptï¼Œæ˜¾ç¤ºå¸®åŠ©
        if not args.prompt:
            print("ğŸ¤– Replicate å¯¹è¯å·¥å…·")
            print("\nå¿«é€Ÿå¼€å§‹:")
            print("  python replicate_chat.py --demo              # è¿è¡Œæ¼”ç¤º")
            print("  python replicate_chat.py --test              # æµ‹è¯•è¿æ¥")
            print("  python replicate_chat.py --list-models       # æŸ¥çœ‹å¯ç”¨æ¨¡å‹")
            print("  python replicate_chat.py --interactive       # äº¤äº’å¼å¯¹è¯")
            print('  python replicate_chat.py "ä½ å¥½ï¼Œä»‹ç»ä¸€ä¸‹è‡ªå·±"   # å•æ¬¡å¯¹è¯')
            print("\næ¨¡å‹é€‰æ‹©:")
            for name, info in CHAT_MODELS.items():
                print(f"  --model {name:<15} # {info['description']}")
            print("\nè¯¦ç»†å¸®åŠ©:")
            print("  python replicate_chat.py -h")
            return

        # å•æ¬¡å¯¹è¯
        response = chat_single(
            prompt=args.prompt,
            model_name=args.model,
            system_prompt=args.system,
            max_tokens=args.max_tokens,
            temperature=args.temperature,
            stream=not args.no_stream,
        )

        print(f"\nğŸ‰ å¯¹è¯å®Œæˆ!")

    except Exception as e:
        print(f"âŒ é”™è¯¯: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main()
