# LLM 微调学习与实践指南

本文档梳理了关于微调小型 LLM 以学习相关技术栈并应用于 RPG 项目的核心结论与决策建议。

## 1. 核心目标与策略

- **策略**：以战代练。不要为了微调而微调，应设定具体业务目标。
- **推荐目标**：微调一个**"RPG 剧情/NPC 扮演专用模型"**。
- **优势**：
  - 数据容易构造（可利用 GPT-4 生成）。
  - 效果直观（语气风格变化明显）。
  - 产出可直接集成到 `ai-rpg` 项目中。

## 2. 模型选型建议

针对个人学习和消费级硬件，推荐 **7B - 8B 参数量** 的模型。

- **首选推荐**：**Qwen2.5-7B-Instruct (通义千问)**
  - *理由*：中文能力极强，指令遵循优秀，非常适合角色扮演。
- **备选方案**：
  - **Llama-3-8B-Instruct**：英文生态最强，社区资源丰富。
  - **Yi-1.5-9B-Chat**：中文语境理解优秀。

## 3. 关键技术栈

- **Unsloth (核心推荐)**：
  - 目前最高效的微调加速库。
  - **优势**：训练速度快 2-5 倍，显存占用减少 50% 以上，且精度无损。
  - *决策*：初学者应直接从 Unsloth 上手，跳过原生 PyTorch/HF Trainer 的复杂配置。
- **PEFT (LoRA/QLoRA)**：
  - 仅训练模型参数的 1%（适配器），冻结主模型。
  - 配合 **Bitsandbytes** 进行 4-bit 量化加载，大幅降低硬件门槛。

## 4. 数据准备 (成败关键)

- **原则**：**数据质量 >> 数据数量**。50 条高质量人工精修数据优于 1000 条低质量数据。
- **格式**：通常为 JSONL 格式，包含 `Instruction` (指令), `Input` (可选上下文), `Output` (期望回答)。
- **内容**：针对 RPG 场景，构造具有特定风格（如黑暗奇幻、中世纪口语）的对话或描述。

## 5. 模型命名后缀详解与选择

理解后缀有助于避免下载错误模型。

### A. Base vs. Instruct (核心分类)

- **Base 模型** (如 `Qwen2.5-7B`)：
  - **本质**："文字接龙"高手，核心能力是预测下一个字。
  - **特点**：不懂指令，只会续写。
  - **适用场景**：拥有海量数据（亿级 token）需要从头灌输知识。
- **Instruct / Chat 模型** (如 `Qwen2.5-7B-Instruct`)：
  - **本质**："懂事的高材生"，经过 SFT (监督微调) 和 RLHF (人类反馈)。
  - **特点**：懂指令，会对话，开箱即用。
  - **决策**：**微调应选择 Instruct 版本作为底座**。因为它已经学会了"如何说话"，我们只需要教它"换一种风格说话"。

### B. 常见后缀速查

- **Chat**：同 Instruct，对话优化版。
- **Code/Coder**：代码能力增强版。
- **VL/Vision**：多模态（支持识图）。
- **128k/1M**：支持超长上下文窗口。

### C. 量化格式 (部署用)

- **GGUF**：**CPU/Mac (M芯片) 首选**。由 `llama.cpp` 定义，适合本地低资源运行。
- **GPTQ/AWQ**：NVIDIA 显卡专用推理优化格式。

## 6. 后续执行路线图

1. **环境准备**：使用 Google Colab (免费 T4) 或本地显卡，安装 `Unsloth`。
2. **数据生成**：利用 GPT-4 生成 50-100 条 RPG 风格对话数据，保存为 JSONL。
3. **模型训练**：加载 `unsloth/Qwen2.5-7B-Instruct`，使用 LoRA 进行微调。
4. **应用集成**：将训练好的 LoRA 权重导出为 **GGUF** 格式，在 `ai-rpg` 项目中通过 `llama.cpp` 或 `vLLM` 加载使用。
