from dotenv import load_dotenv
from loguru import logger

# åŠ è½½ .env æ–‡ä»¶ä¸­çš„ç¯å¢ƒå˜é‡
load_dotenv()

import os
import traceback
from typing import Annotated, Any, Dict, List, Optional

from langchain_core.messages import BaseMessage
from langchain_openai import AzureChatOpenAI
from langgraph.graph import StateGraph
from langgraph.graph.message import add_messages
from langgraph.graph.state import CompiledStateGraph
from pydantic import SecretStr
from typing_extensions import TypedDict


# å…¨å±€Azure OpenAI GPTå®ä¾‹ï¼ˆæ‡’åŠ è½½å•ä¾‹ï¼‰
_global_azure_openai_gpt_llm: Optional[AzureChatOpenAI] = None


def get_azure_openai_gpt_llm() -> AzureChatOpenAI:
    """
    è·å–å…¨å±€Azure OpenAI GPTå®ä¾‹ï¼ˆæ‡’åŠ è½½å•ä¾‹æ¨¡å¼ï¼‰

    Returns:
        AzureChatOpenAI: é…ç½®å¥½çš„Azure OpenAI GPTå®ä¾‹

    Raises:
        ValueError: å½“AZURE_OPENAI_API_KEYç¯å¢ƒå˜é‡æœªè®¾ç½®æ—¶
    """
    global _global_azure_openai_gpt_llm

    if _global_azure_openai_gpt_llm is None:
        logger.info("ğŸ¤– åˆå§‹åŒ–å…¨å±€Azure OpenAI GPTå®ä¾‹...")

        # æ£€æŸ¥å¿…éœ€çš„ç¯å¢ƒå˜é‡
        azure_endpoint = os.getenv("AZURE_OPENAI_ENDPOINT")
        azure_api_key = os.getenv("AZURE_OPENAI_API_KEY")

        if not azure_endpoint:
            raise ValueError("AZURE_OPENAI_ENDPOINT environment variable is not set")

        if not azure_api_key:
            raise ValueError("AZURE_OPENAI_API_KEY environment variable is not set")

        _global_azure_openai_gpt_llm = AzureChatOpenAI(
            azure_endpoint=azure_endpoint,
            api_key=SecretStr(azure_api_key),
            azure_deployment="gpt-4o",
            api_version="2024-02-01",
            temperature=0.7,
        )

        logger.success("ğŸ¤– å…¨å±€DeepSeek LLMå®ä¾‹åˆ›å»ºå®Œæˆ")

    return _global_azure_openai_gpt_llm


############################################################################################################
class State(TypedDict):
    messages: Annotated[List[BaseMessage], add_messages]


############################################################################################################
def create_compiled_stage_graph(
    node_name: str,
) -> CompiledStateGraph[State, Any, State, State]:
    assert node_name != "", "node_name is empty"

    def invoke_azure_chat_openai_llm_action(
        state: State,
    ) -> Dict[str, List[BaseMessage]]:

        try:
            llm = get_azure_openai_gpt_llm()
            assert llm is not None, "Failed to get Azure OpenAI GPT instance"
            return {"messages": [llm.invoke(state["messages"])]}

        except Exception as e:
            logger.error(
                f"Error invoking Azure Chat OpenAI LLM: {e}\n" f"State: {state}"
            )
            traceback.print_exc()
            return {
                "messages": []
            }  # å½“å‡ºç° Azure å†…å®¹è¿‡æ»¤çš„æƒ…å†µï¼Œæˆ–è€…å…¶ä»–ç±»å‹å¼‚å¸¸æ—¶ï¼Œè§†éœ€æ±‚å¯åœ¨æ­¤è¿”å›ç©ºå­—ç¬¦ä¸²æˆ–è€…è‡ªå®šä¹‰æç¤ºã€‚

    graph_builder = StateGraph(State)
    graph_builder.add_node(node_name, invoke_azure_chat_openai_llm_action)
    graph_builder.set_entry_point(node_name)
    graph_builder.set_finish_point(node_name)
    return graph_builder.compile()  # type: ignore[return-value]


############################################################################################################
def stream_graph_updates(
    state_compiled_graph: CompiledStateGraph[State, Any, State, State],
    chat_history_state: State,
    user_input_state: State,
) -> List[BaseMessage]:

    ret: List[BaseMessage] = []

    merged_message_context: State = {
        "messages": chat_history_state["messages"] + user_input_state["messages"]
    }

    for event in state_compiled_graph.stream(merged_message_context):
        for value in event.values():
            ret.extend(value["messages"])

    return ret
