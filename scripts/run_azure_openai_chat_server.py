#!/usr/bin/env python3
"""
Azure OpenAI Chat Serverå¯åŠ¨è„šæœ¬

åŠŸèƒ½ï¼š
1. åŸºäºFastAPIæ„å»ºçš„Azure OpenAI GPTèŠå¤©æœåŠ¡å™¨
2. æä¾›RESTful APIæ¥å£
3. æ”¯æŒèŠå¤©å†å²å’Œä¸Šä¸‹æ–‡è®°å¿†
4. å¼‚æ­¥å¤„ç†èŠå¤©è¯·æ±‚

ä½¿ç”¨æ–¹æ³•ï¼š
    python scripts/run_azure_openai_chat_server.py

æˆ–è€…åœ¨é¡¹ç›®æ ¹ç›®å½•ä¸‹ï¼š
    python -m scripts.run_azure_openai_chat_server

APIç«¯ç‚¹ï¼š
    POST /api/chat/v1/
"""

import os
import sys

# import asyncio
from typing import Any, Dict

# å°† src ç›®å½•æ·»åŠ åˆ°æ¨¡å—æœç´¢è·¯å¾„
sys.path.insert(
    0, os.path.join(os.path.dirname(os.path.abspath(__file__)), "..", "src")
)

from fastapi import FastAPI
from loguru import logger

from ai_rpg.chat_services.protocol import ChatRequest, ChatResponse
from ai_rpg.azure_openai_gpt import (
    create_chat_workflow,
    execute_chat_workflow,
    create_azure_openai_gpt_llm,
)

from ai_rpg.configuration import (
    server_configuration,
)

##################################################################################################################
# åˆå§‹åŒ– FastAPI åº”ç”¨
app = FastAPI(
    title="Azure OpenAI Chat Server",
    description="åŸºäºAzure OpenAI GPTçš„èŠå¤©æœåŠ¡å™¨",
    version="1.0.0",
)


##################################################################################################################
# å¥åº·æ£€æŸ¥ç«¯ç‚¹
@app.get("/")
async def health_check() -> Dict[str, Any]:
    """
    æœåŠ¡å™¨å¥åº·æ£€æŸ¥ç«¯ç‚¹

    Returns:
        dict: åŒ…å«æœåŠ¡å™¨çŠ¶æ€ä¿¡æ¯çš„å­—å…¸
    """
    from datetime import datetime

    return {
        "service": "Azure OpenAI Chat Server",
        "version": "1.0.0",
        "status": "healthy",
        "timestamp": datetime.now().isoformat(),
        "available_endpoints": [
            "GET /",
            "POST /api/chat/v1/",
        ],
        "description": "åŸºäºAzure OpenAIçš„èŠå¤©æœåŠ¡å™¨æ­£åœ¨æ­£å¸¸è¿è¡Œ",
    }


##################################################################################################################
# å®šä¹‰ POST è¯·æ±‚å¤„ç†é€»è¾‘
@app.post(
    path="/api/chat/v1/",
    response_model=ChatResponse,
)
async def process_chat_request(payload: ChatRequest) -> ChatResponse:
    """
    å¤„ç†èŠå¤©è¯·æ±‚

    Args:
        request: åŒ…å«èŠå¤©å†å²å’Œç”¨æˆ·æ¶ˆæ¯çš„è¯·æ±‚å¯¹è±¡

    Returns:
        ChatResponse: åŒ…å«AIå›å¤æ¶ˆæ¯çš„å“åº”å¯¹è±¡
    """
    try:
        logger.info(f"æ”¶åˆ°èŠå¤©è¯·æ±‚: {payload.message.content}")

        # è·å–å›å¤ - ç›´æ¥ await å¼‚æ­¥å‡½æ•°
        chat_response = await execute_chat_workflow(
            work_flow=create_chat_workflow(),
            context=[message for message in payload.chat_history],
            request=payload.message,
            llm=create_azure_openai_gpt_llm(),
        )

        logger.success(f"ç”Ÿæˆå›å¤æ¶ˆæ¯æ•°é‡: {len(chat_response)}")

        # æ‰“å°æ‰€æœ‰æ¶ˆæ¯çš„è¯¦ç»†å†…å®¹
        for i, message in enumerate(chat_response):
            logger.success(f"æ¶ˆæ¯ {i+1}: {message.model_dump_json(indent=2)}")

        # è¿”å›
        return ChatResponse(messages=chat_response)

    except Exception as e:
        logger.error(f"å¤„ç†èŠå¤©è¯·æ±‚æ—¶å‘ç”Ÿé”™è¯¯: {e}")
        return ChatResponse(messages=[])


##################################################################################################################
def main() -> None:
    """
    Azure OpenAIèŠå¤©æœåŠ¡å™¨ä¸»å‡½æ•°

    åŠŸèƒ½ï¼š
    1. å¯åŠ¨FastAPIæœåŠ¡å™¨
    2. é…ç½®æœåŠ¡å™¨å‚æ•°
    3. æä¾›èŠå¤©APIæœåŠ¡
    """
    logger.info("ğŸš€ å¯åŠ¨Azure OpenAIèŠå¤©æœåŠ¡å™¨...")

    try:
        import uvicorn

        # å¯åŠ¨æœåŠ¡å™¨
        uvicorn.run(
            app,
            host="localhost",
            port=server_configuration.azure_openai_chat_server_port,
            log_level="debug",
        )

    except Exception as e:
        logger.error(f"âŒ å¯åŠ¨æœåŠ¡å™¨å¤±è´¥: {e}")
        raise


##################################################################################################################
if __name__ == "__main__":
    main()
